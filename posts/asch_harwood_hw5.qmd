---
title: "Hoemework 5"
author: "Asch Harwood"
description: "Homework 5"
date: "5/9/2023"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - hw1

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 6,
  fig.height = 4,
  fig.align = 'center'
)

library("dplyr")
library("knitr")
library(kableExtra)
library(xtable)
library(ggplot2)
library(GGally)
library(lme4)
library(car)
library(dplyr)
library(alr4)
library(stargazer)
library(smss)
```


# Question 1 

```{r}
data(house.selling.price.2)
```


### background
(Data file: house.selling.price.2 from smss R package)

For the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.

Correlation Matrix:
Price Size Beds Baths
New 0.357 0.176 0.267 0.182 1
      Price 1 Size 0.899 Beds 0.590 Baths 0.714 New 0.357
0.899 0.590 0.714 1 0.669 0.662 0.669 1 0.334 0.662 0.334 1 0.176 0.267 0.182
       
Regression Output (Outcome: House Price):
     (Intercept) -41.795 Size 64.761 Beds -2.766 Baths 19.203 New 18.984
12.104 5.630 3.960 5.650 3.873
Estimate Std. Error
t value -3.453 11.504 -0.698 3.399 4.902
Pr(> | t| ) 0.001
0
0.487 0.001 0.00000


 (Hint 1: You should be able to answer A, B, C just using the tables below, although you should feel free to load the data in R and work with it if you so choose. They will be consistent with what you see on the tables.
Hint 2: The p-value of a variable in a simple linear regression is the same p-value one would get from a Pearson’s correlation (cor.test). The p-value is a function of the magnitude of the correlation coefficient (the higher the coefficient, the lower the p-value) and of sample size (larger samples lead to smaller p-values). For the correlations shown in the tables, they are between variables of the same length.)

### A

Beds would be deleted first. It has the highest p-value of 0.487, which also happens to not be statistically significant.

### B

The first fit would be an intercept only model, which means there is no explanatory variable. This becomes our 'baseline' against which we can evaluate our model as we added explanatory variables.

### C

C. Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?

Beds also has relatively strong relationship with size, which has a strong relationship with size. This means that this current model suffers multicollinearity, which is obscuring the potential relationship between size and price. It makes sense that number of beds would correlate with both the size of the house and the price of the house. 

### D

4. AIC
5. BIC

```{r}
fit_S <- lm(P ~ S + Ba + New, data = house.selling.price.2)
fit_Be <- lm(P ~ S + New, data = house.selling.price.2)
fit_Ba <- lm(P ~ New, data = house.selling.price.2)
fit_New <- lm(P ~ 1, data = house.selling.price.2)
```


#### R2

- With an R2 of 0.87: S + Ba + New

```{r}
summary(fit_S)$r.squared
summary(fit_Be)$r.squared
summary(fit_Ba)$r.squared
summary(fit_New)$r.squared
```
#### Adjusted R2

- With an Adjusted R2 of 0.86: S + Ba + New

```{r}
summary(fit_S)$adj.r.squared
summary(fit_Be)$adj.r.squared
summary(fit_Ba)$adj.r.squared
summary(fit_New)$adj.r.squared
```
#### PRESS

- Again, S + Ba + New, has the smallest PRESS, which means it has the best 'predictive' power compared to the other models

```{r}
press_stat <- function(model) {
  # Calculate PRESS residuals
  pr <- resid(model) / (1 - lm.influence(model)$hat)
  
  # Compute the PRESS statistic
  press <- sum(pr^2)
  
  return(press)
}
```

```{r}
press_stat(fit_S)
press_stat(fit_Be)
press_stat(fit_Ba)
press_stat(fit_New)
```
#### AIC

- Again, S + Ba + New has the smallest AIC, which suggests it does a better job of fitting the data without overfitting

```{r}
AIC(fit_S)
AIC(fit_Be)
AIC(fit_Ba)
AIC(fit_New)
```
#### BIC
- Again, S + Ba + New has the smallest BIC, which suggests it does a better job of fitting the data without overfitting

```{r}
BIC(fit_S)
BIC(fit_Be)
BIC(fit_Ba)
BIC(fit_New)
```

### E

I prefer P ~ S + Ba + New. All coefficients are statistically significant. It outperforms the 'less complex' models on all metrics. It also make sense that several different factors influence home price.

# Question 2

```{r}
data(trees)
head(trees)
str(trees)
```

### background 
(Data file: trees from base R)
From the documentation:
“This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.”
Tree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,

### A

A. Fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables

```{r}
fit <- lm(Volume ~ Girth + Height, data = trees)
summary(fit)
```


### B


**Residuals vs Fitted**

The curved shape of the line suggests this model violates our assumption of linearity that the relationship between the independent and dependent variables is linear.

**Scale-Location*

The curved shape of the line suggests this model violates our assumption of constant variance, or homoskedacity, which affects the statisical significance of the model.

**Cooks Distance,  Residuals vs Leverage, Cooks dist vs Leverage**

- All three charts show there is single, potentially influence point, which can impact whether our model meets the assumptions of linear regression and can disproportionately affect our regression coefficients. 

```{r cache=TRUE}
par(mfrow = c(2,3))
plot(fit, which = 1:6)
```


# Question 3
### background

(Data file: florida in alr R package)

In the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.
The data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan.

### A

A. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?

### B

B. Take the log of both variables (Bush vote and Buchanan Vote) and repeat the analysis in (A.) Does your findings change?








